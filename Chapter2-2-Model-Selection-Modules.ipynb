{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# In this section, we will check \"train_test_split\"\n",
    "iris = load_iris()\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "train_data = iris.data\n",
    "train_label = iris.target\n",
    "dt_clf.fit(train_data, train_label)\n",
    "\n",
    "# Prediction based on trained data set\n",
    "pred = dt_clf.predict(train_data)\n",
    "print(\"Prediction accuracy:\", accuracy_score(train_label, pred))\n",
    "\n",
    "# The reason we got \"1.0\", \"100%\" is that we train and predict with the same data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"test_size\" is to allocate the test set size (default is 25% (0.25))\n",
    "# \"train_size\" is not commonly used becuase we usually use \"test_size\"\n",
    "# \"random_stata\", if we don't specify it, it will be ramdomized to split the data (train and test)\n",
    "# \"train_test_split()\" returns Tuple types and data types in order feature data set of training data, feature data set of testing data, label data set of training data set, and label data set of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "iris_data = load_iris()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.3, random_state=121)\n",
    "\n",
    "dt_clf.fit(X_train,y_train)\n",
    "pred = dt_clf.predict(X_test)\n",
    "print(\"Prediction accuracy: {0:.4f}\".format(accuracy_score(y_test, pred)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation is to take pre-SAT tests for SAT\n",
    "# This is also to avoid the overfitting issue\n",
    "# k-fold cross validation divides the data set into \"k\" and with the same range of \"k\" numbers of data set, data sets (1 to k-1) is used to train and evaluated by the data set, \"k\". \n",
    "# And it repeats \"k\" times by changing the test and train data sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "features = iris.data\n",
    "label = iris.target\n",
    "dt_clf = DecisionTreeClassifier(random_state=156)\n",
    "\n",
    "# it is to do k=5\n",
    "kfold = KFold(n_splits=5)\n",
    "cv_accuracy = []\n",
    "# \".shpae[n]\" shows you the n-axis (0= number of rows, 1= number of columns)\n",
    "print(\"Size of iris data set:\", features.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 0\n",
    "\n",
    "# If call \"split()\" from KFold object, it returns training and testing of row index per \"k\" as array\n",
    "for train_index, test_index in kfold.split(features):\n",
    "    # Using returned \"kfold.split()\" index, extract training and testing data \n",
    "    X_train, X_test = features[train_index], features[test_index]\n",
    "    y_train, y_test = label[train_index], label[test_index]\n",
    "    \n",
    "    # Train and predict\n",
    "    dt_clf.fit(X_train, y_train)\n",
    "    pred = dt_clf.predict(X_test)\n",
    "    n_iter += 1\n",
    "\n",
    "    # Check accuracy per interation\n",
    "    accuracy = np.round(accuracy_score(y_test, pred), 4)\n",
    "    train_size = X_train.shape[0]\n",
    "    test_size = X_test.shape[0]\n",
    "    print(\"\\n#{0} Cross validation accuracy: {1}, Size of training data: {2}, Size of testing data: {3}\".format(n_iter, accuracy, train_size, test_size))\n",
    "    print(\"#{0} Testing data index:{1}\".format(n_iter, test_index))\n",
    "    cv_accuracy.append(accuracy)\n",
    "\n",
    "# Calculate the average accuracy by using each iteration result\n",
    "print(\"\\n## Average validation accuracy:\", np.mean(cv_accuracy))\n",
    "print(cv_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified K fold, \"StratifiedKFold\", is to predict an occurance in something rarely happened (within 100,000,000 data points, there is only 2-5 phishing scams and with K fold apporoach, most of k data set says not likely happen)\n",
    "# To avoid this, we introduce \"StratifiedKFold\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "iris_df['label'] = iris.target\n",
    "iris_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the code below, we can't train and predict the value (0-2) because in 1 and 2, there is no feature for 0\n",
    "kfold = KFold(n_splits=3)\n",
    "n_iter = 0\n",
    "for train_index, test_index in kfold.split(iris_df):\n",
    "    n_iter+=1\n",
    "    label_train=iris_df['label'].iloc[train_index]\n",
    "    label_test=iris_df['label'].iloc[test_index]\n",
    "    print(\"## Cross Validation: {0}\".format(n_iter))\n",
    "    print(\"Distribution of Training labeled data:\\n\", label_train.value_counts())\n",
    "    print(\"Distribution of Testing labeled data:\\n\", label_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "n_iter = 0\n",
    "\n",
    "for train_index, test_index in skf.split(iris_df, iris_df[\"label\"]):\n",
    "    n_iter+=1\n",
    "    label_train=iris_df[\"label\"].iloc[train_index]\n",
    "    label_test=iris_df[\"label\"].iloc[test_index]\n",
    "    print(\"## Cross Validation: {0}\".format(n_iter))\n",
    "    print(\"Distribution of Training labeled data:\\n\", label_train.value_counts())\n",
    "    print(\"Distribution of Testing labeled data:\\n\", label_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf = DecisionTreeClassifier(random_state=156)\n",
    "\n",
    "skfold = StratifiedKFold(n_splits=3)\n",
    "n_iter = 0\n",
    "cv_accuracy = []\n",
    "\n",
    "# It is important that we need Label data set to be typed in to use split()\n",
    "for train_index, test_index in skfold.split(features, label):\n",
    "    # Based on index from split(), export tesing and training data\n",
    "    X_train, X_test = features[train_index], features[test_index]\n",
    "    y_train, y_test = label[train_index], label[test_index]\n",
    "    # Train and predict\n",
    "    dt_clf.fit(X_train, y_train)\n",
    "    pred = dt_clf.predict(X_test)\n",
    "\n",
    "    # Accuracy per iteration\n",
    "    n_iter+=1\n",
    "    accuracy=np.round(accuracy_score(y_test, pred), 4)\n",
    "    train_size = X_train.shape[0]\n",
    "    test_size = X_test.shape[0]\n",
    "    print(\"\\n#{0} Cross validation accuracy: {1}, Size of traing data: {2}, Size of testing data: {3}\".format(n_iter, accuracy, train_size, test_size))\n",
    "    print(\"#{0} Testing data index:{1}\".format(n_iter, test_index))\n",
    "    cv_accuracy.append(accuracy)\n",
    "\n",
    "# Accuracy per crosss validation and average accuracy of validation\n",
    "print(\"\\n## Accuracy per cross validation:\", np.round(cv_accuracy, 4))\n",
    "print(\"## Average validation accuracy:\", np.round(np.mean(cv_accuracy), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris_data = load_iris()\n",
    "dt_clf = DecisionTreeClassifier(random_state=156)\n",
    "\n",
    "data = iris_data.data\n",
    "label = iris_data.target\n",
    "\n",
    "# Score indicator is accuracy, three sets of cross validation\n",
    "scores = cross_val_score(dt_clf,data, label, scoring='accuracy', cv=3)\n",
    "print('Cross validation accuracy:', np.round(scores,4))\n",
    "print('Average validation accuracy:', np.round(np.mean(scores),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_parameters = {'max_depth': [1,2,3], 'min_samples_split': [2,3]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load the data and split the data between training set and testing set\n",
    "iris_data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.2, random_state=121)\n",
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "# Set parameters as disctionary type\n",
    "parameters = {'max_depth':[1,2,3], 'min_samples_split':[2,3]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set up test execution by dividing hyperparameters of param_grid into 3 set folds: train and test\n",
    "# refit=True is a default. When it is True, re-train with the best parameter\n",
    "grid_dtree = GridSearchCV(dtree, param_grid=parameters, cv=3, refit=True)\n",
    "\n",
    "# With Iris train data, train and test hyperparameter of param_grid in order\n",
    "grid_dtree.fit(X_train, y_train)\n",
    "\n",
    "# By extracting GridSearchCV results, covert to DataFrame\n",
    "score_df = pd.DataFrame(grid_dtree.cv_results_)\n",
    "score_df[['params', 'mean_test_score', 'rank_test_score', 'split0_test_score', 'split1_test_score', 'split2_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GridSearchCV optimal parameter:', grid_dtree.best_params_)\n",
    "print('Best accuracy of GridSearchCV:{0:.4f}'.format(grid_dtree.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on refitted GridSearchCV, return the trained estimator\n",
    "estimator = grid_dtree.best_estimator_\n",
    "\n",
    "# No need a further train for best_estimator_ of GridSearchCV because it is already optimized \n",
    "pred = estimator.predict(X_test)\n",
    "print('Test data set accuracy: {0:.4f}'.format(accuracy_score(y_test, pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
